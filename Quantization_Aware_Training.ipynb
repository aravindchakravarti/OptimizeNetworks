{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aravindchakravarti/OptimizeNetworks/blob/main/Quantization_Aware_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reference Code Available at: \n",
        "\n",
        "- Quantization Aware Training: https://www.tensorflow.org/model_optimization/guide/quantization/training_example\n",
        "\n",
        "- Post-training Quantization: https://www.tensorflow.org/lite/performance/post_training_quant"
      ],
      "metadata": {
        "id": "L4xnUtBKN_kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dependencies and Import Libraries\n"
      ],
      "metadata": {
        "id": "DlPkZRZsN7ll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zN4yVFK5-0Bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512a806f-0c10-4bde-f49c-ef4e439bbde1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 143 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 174 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 204 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 225 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 235 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 238 kB 10.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -q tensorflow\n",
        "! pip install -q tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yJwIonXEVJo6"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from time import perf_counter\n",
        "from statistics import mean\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psViY5PRDurp"
      },
      "source": [
        "# Build a MNIST Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Design and Training"
      ],
      "metadata": {
        "id": "IT8DJb_bO9v9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pbY-KGMPvbW9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c0b5d0-3527-4b6a-a8b3-7390b239a0ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "1688/1688 [==============================] - 18s 5ms/step - loss: 0.3168 - accuracy: 0.9082 - val_loss: 0.1335 - val_accuracy: 0.9655\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f50800720d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Define the model architecture.\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=1,\n",
        "  validation_split=0.1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference of Model"
      ],
      "metadata": {
        "id": "rYML10KVPC_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_time = []\n",
        "for i in range (10):\n",
        "  start = perf_counter()\n",
        "  model.evaluate(test_images, test_labels)\n",
        "  stop = perf_counter()\n",
        "  inference_time.append(stop-start)\n",
        "  \n",
        "for i in range(10):\n",
        "  print(\"Inference Time Diff = \", inference_time[i])\n",
        "\n",
        "print(\"Mean Time Diff = \", mean(inference_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PA5amkKFNV-",
        "outputId": "230cd549-bea7-4f57-e3e6-9c073da3b976"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1510 - accuracy: 0.9570\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1510 - accuracy: 0.9570\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1510 - accuracy: 0.9570\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1510 - accuracy: 0.9570\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1510 - accuracy: 0.9570\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1510 - accuracy: 0.9570\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1510 - accuracy: 0.9570\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1510 - accuracy: 0.9570\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1510 - accuracy: 0.9570\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1510 - accuracy: 0.9570\n",
            "Inference Time Diff =  1.334142503999999\n",
            "Inference Time Diff =  0.9253799219999905\n",
            "Inference Time Diff =  0.798293238000042\n",
            "Inference Time Diff =  1.3292515269999967\n",
            "Inference Time Diff =  1.0805777310000053\n",
            "Inference Time Diff =  1.3261469970000235\n",
            "Inference Time Diff =  1.3384186769999928\n",
            "Inference Time Diff =  1.3260544179999556\n",
            "Inference Time Diff =  1.3274303059999966\n",
            "Inference Time Diff =  0.7841694370000027\n",
            "Mean Time Diff =  1.1569864757000006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8747K9OE72P"
      },
      "source": [
        "# Quantization of Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert the model in Tensorflow lite\n",
        "\n",
        "After applying quantization aware training to the whole model we can see the model summary. All layers are now prefixed by \"quant\".\n",
        "\n",
        "Note that the resulting model is quantization aware but not quantized (e.g. the weights are float32 instead of int8). Next sections show how to create a quantized model from the quantization aware one.\n",
        "\n",
        "In the [comprehensive guide](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide.md),"
      ],
      "metadata": {
        "id": "uW8KJ69vPKbZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oq6blGjgFDCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08ada3f-26fc-4925-e93f-b8a3293fe51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLay  (None, 28, 28)           3         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " quant_reshape (QuantizeWrap  (None, 28, 28, 1)        1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrapp  (None, 26, 26, 12)       147       \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quanti  (None, 13, 13, 12)       1         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWrap  (None, 2028)             1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrappe  (None, 10)               20295     \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,448\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 38\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for for quantization aware.\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDr2ijwpGCI-"
      },
      "source": [
        "## Psuedo-Transfer-Learning\n",
        "Kind of a transfer learning. Re-train the model as we did model \"Quantization Aware\" for some epoch. To demonstrate fine tuning after training the model for just an epoch, fine tune with quantization aware training on a subset of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_PHDGJryE31X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57070d9a-e2f3-4c0e-f071-56ebce2145c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 264ms/step - loss: 0.1576 - accuracy: 0.9578 - val_loss: 0.1707 - val_accuracy: 0.9500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5032149b50>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_images_subset = train_images[0:1000] # out of 60000\n",
        "train_labels_subset = train_labels[0:1000]\n",
        "\n",
        "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
        "                  batch_size=500, epochs=1, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-byC2lYlMkfN"
      },
      "source": [
        "For this example, there is minimal to no loss in test accuracy after quantization aware training, compared to the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6bMFTKSSHyyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4a81b0-74f0-4452-d011-2ef353312161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline test accuracy: 0.9570000171661377\n",
            "Quant test accuracy: 0.9563999772071838\n"
          ]
        }
      ],
      "source": [
        "_, baseline_model_accuracy = model.evaluate(\n",
        "    test_images, test_labels, verbose=0)\n",
        "\n",
        "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
        "   test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Quant test accuracy:', q_aware_model_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IepmUPSITn6"
      },
      "source": [
        "## Interpreter\n",
        "\n",
        "We do not have edge device with us. Hence, we need some kind of emulation to run tensorflow lite model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b8yBouuGNqls"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    #if i % 1000 == 0:\n",
        "    #  print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  #print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  FLOAT-32 BIT Quantizing"
      ],
      "metadata": {
        "id": "FdTfCyUoQCkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter_f32 = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# No optimiser speicification\n",
        "# No supported_types \n",
        "tflite_model_f32 = converter_f32.convert()\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_f32)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "inference_time = []\n",
        "for i in range (10):\n",
        "  start = perf_counter()\n",
        "  test_accuracy = evaluate_model(interpreter)\n",
        "  stop = perf_counter()\n",
        "  inference_time.append(stop-start)\n",
        "  \n",
        "for i in range(10):\n",
        "  print(\"Inference Time Diff = \", inference_time[i])\n",
        "\n",
        "print(\"Mean Time Diff = \", mean(inference_time))\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I63xM7ZYMOWZ",
        "outputId": "7a4d005a-fbf3-4a3e-c64d-2d101c317e91"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time Diff =  1.3139581130000124\n",
            "Inference Time Diff =  1.3142963470000382\n",
            "Inference Time Diff =  1.3004703240000026\n",
            "Inference Time Diff =  1.312723329999983\n",
            "Inference Time Diff =  1.3041038720000415\n",
            "Inference Time Diff =  1.3025300639999955\n",
            "Inference Time Diff =  1.2839879800000062\n",
            "Inference Time Diff =  1.3104669129999706\n",
            "Inference Time Diff =  1.3276079019999543\n",
            "Inference Time Diff =  1.2687385360000007\n",
            "Mean Time Diff =  1.3038883381000006\n",
            "Quant TFLite test_accuracy: 0.957\n",
            "Quant TF test accuracy: 0.9563999772071838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEYsyYVqNgeY"
      },
      "source": [
        "## FLOAT-16 BIT Quantizing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter_fl16 = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter_fl16.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter_fl16.target_spec.supported_types = [tf.float16]\n",
        "quantized_tflite_model_f16 = converter_fl16.convert()\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model_f16)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "inference_time = []\n",
        "for i in range (10):\n",
        "  start = perf_counter()\n",
        "  test_accuracy = evaluate_model(interpreter)\n",
        "  stop = perf_counter()\n",
        "  inference_time.append(stop-start)\n",
        "  \n",
        "for i in range(10):\n",
        "  print(\"Inference Time Diff = \", inference_time[i])\n",
        "\n",
        "print(\"Mean Time Diff = \", mean(inference_time))\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-R8-jHirkaf",
        "outputId": "df645e85-b48c-4877-ed4d-62fea9c4a2ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as reshape_layer_call_fn, reshape_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op while saving (showing 5 of 9). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time Diff =  0.6119103119999636\n",
            "Inference Time Diff =  0.5931340540000178\n",
            "Inference Time Diff =  0.5934894220000047\n",
            "Inference Time Diff =  0.6123927529999946\n",
            "Inference Time Diff =  0.5740142430000219\n",
            "Inference Time Diff =  0.5876860340000007\n",
            "Inference Time Diff =  0.5845103400000085\n",
            "Inference Time Diff =  0.6354716249999797\n",
            "Inference Time Diff =  0.6081682299999898\n",
            "Inference Time Diff =  0.6215319369999861\n",
            "Mean Time Diff =  0.6022308949999967\n",
            "Quant TFLite test_accuracy: 0.9564\n",
            "Quant TF test accuracy: 0.9563999772071838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  INT-8 BIT Quantizing"
      ],
      "metadata": {
        "id": "eAdrA5TOQQT2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w7fztWsAOHTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b384a8cc-1842-4b3b-baf8-cd83fe2de0d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as reshape_layer_call_fn, reshape_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op while saving (showing 5 of 9). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time Diff =  0.6110938000000488\n",
            "Inference Time Diff =  0.5803180810000299\n",
            "Inference Time Diff =  0.5870856820000085\n",
            "Inference Time Diff =  0.597313865999979\n",
            "Inference Time Diff =  0.6075698110000189\n",
            "Inference Time Diff =  0.5936725999999908\n",
            "Inference Time Diff =  0.598558799999978\n",
            "Inference Time Diff =  0.5820988860000398\n",
            "Inference Time Diff =  0.5880661200000077\n",
            "Inference Time Diff =  0.5829093030000081\n",
            "Mean Time Diff =  0.5928686949000109\n",
            "Quant TFLite test_accuracy: 0.9564\n",
            "Quant TF test accuracy: 0.9563999772071838\n"
          ]
        }
      ],
      "source": [
        "converter_t8 = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter_t8.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model_t8 = converter_t8.convert()\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model_t8)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "inference_time = []\n",
        "for i in range (10):\n",
        "  start = perf_counter()\n",
        "  test_accuracy = evaluate_model(interpreter)\n",
        "  stop = perf_counter()\n",
        "  inference_time.append(stop-start)\n",
        "  \n",
        "for i in range(10):\n",
        "  print(\"Inference Time Diff = \", inference_time[i])\n",
        "\n",
        "print(\"Mean Time Diff = \", mean(inference_time))\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing Memory"
      ],
      "metadata": {
        "id": "6-VP4mFBzvxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_models_dir = pathlib.Path(\"./mnist_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "tflite_model_file = tflite_models_dir/\"mnist_model_f32.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model_f32)\n",
        "\n",
        "tflite_model_file = tflite_models_dir/\"mnist_model_quant_f16.tflite\"\n",
        "tflite_model_file.write_bytes(quantized_tflite_model_f16)\n",
        "\n",
        "tflite_model_file = tflite_models_dir/\"mnist_model_quant_t8.tflite\"\n",
        "tflite_model_file.write_bytes(quantized_tflite_model_t8)\n",
        "\n",
        "!ls -lh {tflite_models_dir}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWp8L56DFyMo",
        "outputId": "e70f09ef-2807-41ec-edc9-d0707edb7c0d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 140K\n",
            "-rw-r--r-- 1 root root 83K Dec  6 11:52 mnist_model_f32.tflite\n",
            "-rw-r--r-- 1 root root 25K Dec  6 11:52 mnist_model_quant_f16.tflite\n",
            "-rw-r--r-- 1 root root 25K Dec  6 11:52 mnist_model_quant_t8.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jy_Lgfh8VkyX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "47519519-604e-4c54-a028-85adff83604f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Create float TFLite model.\\nfloat_converter = tf.lite.TFLiteConverter.from_keras_model(model)\\nfloat_tflite_model = float_converter.convert()\\n\\n# Measure sizes of models.\\n_, float_file = tempfile.mkstemp(\\'.tflite\\')\\n_, quant_file = tempfile.mkstemp(\\'.tflite\\')\\n_, quant_file_f16 = tempfile.mkstemp(\\'.tflite\\')\\n\\nwith open(quant_file, \\'wb\\') as f:\\n  f.write(quantized_tflite_model)\\n\\nwith open(float_file, \\'wb\\') as f:\\n  f.write(float_tflite_model)\\n\\nwith open(quant_file_f16, \\'wb\\') as f:\\n  f.write(quantized_tflite_model_f16)\\n\\nprint(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\\nprint(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))\\nprint(\"Float f16 model in Mb:\", os.path.getsize(quant_file_f16) / float(2**20))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "'''\n",
        "# Create float TFLite model.\n",
        "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "float_tflite_model = float_converter.convert()\n",
        "\n",
        "# Measure sizes of models.\n",
        "_, float_file = tempfile.mkstemp('.tflite')\n",
        "_, quant_file = tempfile.mkstemp('.tflite')\n",
        "_, quant_file_f16 = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(quant_file, 'wb') as f:\n",
        "  f.write(quantized_tflite_model)\n",
        "\n",
        "with open(float_file, 'wb') as f:\n",
        "  f.write(float_tflite_model)\n",
        "\n",
        "with open(quant_file_f16, 'wb') as f:\n",
        "  f.write(quantized_tflite_model_f16)\n",
        "\n",
        "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
        "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))\n",
        "print(\"Float f16 model in Mb:\", os.path.getsize(quant_file_f16) / float(2**20))\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}